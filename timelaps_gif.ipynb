{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7a1862",
   "metadata": {},
   "source": [
    "# Titel \n",
    "\n",
    "## Abstract\n",
    "\n",
    "\n",
    "## Requirements\n",
    "`Python 3.14.2`\n",
    "\n",
    "| Package       | Version   |\n",
    "| ------------- | --------- |\n",
    "| ImageIO       | 2.37.2    |\n",
    "| matplotlib    | 3.10.8    |\n",
    "| numpy         | 2.2.6     |\n",
    "| opencv-python | 4.12.0.88 |\n",
    "| scikit-image  | 0.26.0    |\n",
    "\n",
    "```bash\n",
    "pip install imageio==2.37.2 matplotlib==3.10.8 numpy==2.2.6 opencv-python==4.12.0.88 scikit-image==0.26.0\n",
    "```\n",
    "\n",
    "## Note\n",
    "The following function is for visualisation purposes only and has no influence on the actual data processing. To use it throughout the notebook, the corresponding code block must be executed once.\n",
    "After restarting the kernel or the project, all previously defined functions are lost. In this case, this code block must always be executed first before subsequent cells function correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2c04c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://stackoverflow.com/questions/3173320/text-progress-bar-in-terminal-with-block-characters\n",
    "def progressBar(iterable, total, prefix='', suffix='', decimals=1, length=100, fill='█', printEnd=\"\\r\"):\n",
    "    def printProgressBar(iteration):\n",
    "        percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "        filledLength = int(length * iteration // total)\n",
    "        bar = fill * filledLength + '-' * (length - filledLength)\n",
    "        print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end=printEnd)\n",
    "    \n",
    "    printProgressBar(0)\n",
    "    \n",
    "    for i, item in enumerate(iterable):\n",
    "        yield item\n",
    "        printProgressBar(i + 1)\n",
    "        \n",
    "    # print new line on complete\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db43a42",
   "metadata": {},
   "source": [
    "## Preperation - Extraction of subframes\n",
    "During the design sessions, the participants' interactions with CollabJam were recorded via screen capture. Since analysing all frames (in this specific case, approximately 57 minutes of playback time at 24 frames per second) would be too time-consuming and possibly pointless, subframes are extracted in the first step. \n",
    "\n",
    "The `interval` parameter can be used to control how many subframes per minute are to be saved. Our analysis uses 8 frames per minute here. The complete extraction may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "595a43d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames: |████████████████████████████████████████| 100.0% Complete\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# paths\n",
    "video_path = r\"Assets\\screenrecording_designsession.mp4\"\n",
    "output_folder = r\"Output\\extracted_subframes\"\n",
    "\n",
    "# 1/x * 60 --> x Frames extracted per minute\n",
    "interval = 1/8 * 60\n",
    "\n",
    "# generate timestamp (MM:SS)\n",
    "def format_time(seconds):\n",
    "    mins = int(seconds) // 60\n",
    "    secs = int(seconds) % 60\n",
    "    return f\"{mins:02d}:{secs:02d}\"\n",
    "\n",
    "# extract frames\n",
    "def extract_screenshots(video_path, output_folder, interval):\n",
    "    # gather metadata\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / fps\n",
    "\n",
    "    screenshots = []\n",
    "\n",
    "    num_shots = int(duration // interval)\n",
    "\n",
    "    # extract frames\n",
    "    for i in progressBar(range(num_shots + 1), total=num_shots + 1, prefix='Extracting frames:', suffix='Complete', length=40):\n",
    "        timestamp = i * interval\n",
    "        frame_number = int(timestamp * fps)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # create timestamp text\n",
    "        text = f\"Zeit: {format_time(timestamp)} / {format_time(duration)}\"\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1\n",
    "        thickness = 2\n",
    "        color = (255, 255, 255)\n",
    "        outline = (0, 0, 0)\n",
    "        pos = (30, 50)\n",
    "        cv2.putText(frame, text, pos, font, font_scale, outline, thickness + 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, text, pos, font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "        # save\n",
    "        filename = os.path.join(output_folder, f\"screenshot_{frame_number}.png\")\n",
    "        cv2.imwrite(filename, frame)\n",
    "        screenshots.append(filename)\n",
    "\n",
    "    cap.release()\n",
    "    return screenshots\n",
    "\n",
    "def main():\n",
    "    # check if output_folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    extract_screenshots(video_path, output_folder, interval)\n",
    "    print(f\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1410fc33",
   "metadata": {},
   "source": [
    "## Processing and analysis of subframes\n",
    "\n",
    "The analysis procedure comprised several consecutive steps. First, the previously extracted subframes were cropped using a mask to isolate only the relevant area of the user interface for analysis. \n",
    "\n",
    "### 1. Masking\n",
    "\n",
    "The mask was created manually in an image editing programme and depends on the respective source material. White areas of the mask remain in the image after masking, while areas marked in black are removed or appear black in the masked image.\n",
    "It is also important that the dimensions of the mask are identical to those of the extracted subframes.\n",
    "\n",
    "<figure id=\"Fig.1_mask\" style=\"--min-width: 15em\">\n",
    "    <figure>\n",
    "        <img src=\"Assets\\mask.png\"/>\n",
    "        <figcaption>mask</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\00_subframe.png\"/>\n",
    "        <figcaption>original subframe</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\01_masked.png\"/>\n",
    "        <figcaption>masked subframe</figcaption>\n",
    "    </figure>\n",
    "    <figcaption><strong>Figure 1: masking of the original subframe</strong></figcaption>\n",
    "</figure>\n",
    "\n",
    "### 2. Thresholding\n",
    "\n",
    "In the next step, the cropped images were first converted to greyscale and then transformed into binary images.\n",
    "\n",
    "<figure id=\"Fig.2_thresholding\" style=\"--min-width: 15em\">\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\01_masked.png\"/>\n",
    "        <figcaption>result of the prevoius steps (masking)</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\02_thresholding.png\"/>\n",
    "        <figcaption>result of thresholding</figcaption>\n",
    "    </figure>\n",
    "    <figcaption><strong>Figure 2: thresholding</strong></figcaption>\n",
    "</figure>\n",
    "\n",
    "### 3. Morphological opening\n",
    "Since this representation still contained unwanted image elements such as track markings, a morphological opening was applied to remove these disturbances and clean up the image structure.\n",
    "\n",
    "<figure id=\"Fig.3_morphology\" style=\"--min-width: 15em\">\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\02_thresholding.png\"/>\n",
    "        <figcaption>result of the prevoius steps (masking, thresholding)</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\03_morphology.png\"/>\n",
    "        <figcaption>after morphology</figcaption>\n",
    "    </figure>\n",
    "    <figcaption><strong>Figure 3: morphological opening</strong></figcaption>\n",
    "</figure>\n",
    "\n",
    "### 4. Contour detection\n",
    "This was followed by the detection of the block contours.\n",
    "\n",
    "<figure id=\"Fig.4_contours\" style=\"--min-width: 15em\">\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\03_morphology.png\"/>\n",
    "        <figcaption>result of the prevoius steps (masking, thresholding, morphology)</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\04.1_contours_only.png\"/>\n",
    "        <figcaption>detected contours</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\04.2_contours_with_original.png\"/>\n",
    "        <figcaption>dectected contours with original subframe</figcaption>\n",
    "    </figure>\n",
    "    <figcaption><strong>Figure 4: contour detection</strong></figcaption>\n",
    "</figure>\n",
    "\n",
    "### 5. Lifetime map\n",
    "Based on these contours, a so-called lifetime map was created, a data structure that assigns a lifetime value to each detected pixel from the contour detection. New or significantly changed block contours appear as brightly highlighted areas that fade over time. This logic creates a visual representation of the dynamics in the design process. The GIF animation created shows the development of the interactions over time by clearly highlighting changes in the timeline. In this way, the creative work process can be depicted intuitively and comprehensibly.\n",
    "\n",
    "<figure id=\"Fig.5_lifetime_map\" style=\"--min-width: 15em\">\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\04.1_contours_only.png\"/>\n",
    "        <figcaption>result of the prevoius steps (masking, thresholding, morphology, contour detection)</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\05_lifetime_map_after_first_frame.png\"/>\n",
    "        <figcaption>lifetime map after the first frame</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\05_lifetime_map_after_some_frames.png\"/>\n",
    "        <figcaption>lieftime map after some frames have been processed</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\06_overlay_progress.gif\"/>\n",
    "        <figcaption>final animation</figcaption>\n",
    "    </figure>\n",
    "    <figcaption><strong>Figure 5: lifetime map</strong></figcaption>\n",
    "</figure>\n",
    "\n",
    "### 6. Quantitative analysis\n",
    "In addition to visual analysis, the design process was also evaluated quantitatively using the <a href=\"https://scikit-image.org/docs/0.25.x/auto_examples/transform/plot_ssim.html\">Structural Similarity Index (SSIM)</a>. This value indicates how similar two images are in terms of their structure based on perception. In contrast to simple error measures (e.g. MSE), SSIM not only evaluates pixel-by-pixel differences, but also focuses on characteristics that are relevant to human vision. \n",
    "\n",
    "The SSIM ultimately calculates a dimensionless scalar value that indicates how structurally similar two images are. The value typically ranges between 0 and 1, with 1 representing nearly identical images and smaller values indicating increasing structural differences.\n",
    "In this use case, however, it is not the similarity but the visual change between successive frames that is of interest. Therefore, the inverse of the SSIM (1 − SSIM) is used. High values mark points in time with significant changes, while low values indicate phases with little or no activity. This allows changes over time to be clearly and intuitively identified in the diagram. In this context, the inverse is referred to as the **SSIM activity** or **visual activity**.\n",
    "\n",
    "#### 6.1 SSIM activity diagram\n",
    "The calculation was performed for all consecutive frames of the generated animation and the results were visualised in a diagram that shows the dynamics of change over the entire period.\n",
    "\n",
    "<figure id=\"Fig.5_lifetime_map\" style=\"--min-width: 15em\">\n",
    "    <img src=\"Assets\\images_for_markdown\\activity_diagram_ssim.svg\"/>    \n",
    "    <figcaption><strong>Figure 6: SSIM-activity diagram</strong></figcaption>\n",
    "</figure>\n",
    "\n",
    "#### 6.2 Excerpt from the list of highest SSIM activity values\n",
    "In addition, a sorted list was generated that lists the frame pairs with the highest values, including the corresponding frame numbers and file names, enabling targeted tracing of the most significant moments of change. \n",
    "\n",
    "| Platz | SSIM-activity  | Frame A | File A                | Frame B | File B                |\n",
    "|-------|----------------|---------|-----------------------|---------|-----------------------|\n",
    "| 1     | 0.007          | 350     | screenshot_64440.png  | 351     | screenshot_64620.png  |\n",
    "| 2     | 0.007          | 13      | screenshot_3420.png   | 14      | screenshot_3600.png   |\n",
    "| 3     | 0.005          | 349     | screenshot_64260.png  | 350     | screenshot_64440.png  |\n",
    "| 4     | 0.004          | 357     | screenshot_65700.png  | 358     | screenshot_66240.png  |\n",
    "| 5     | 0.004          | 185     | screenshot_34560.png  | 186     | screenshot_34740.png  |\n",
    "| 6     | 0.003          | 40      | screenshot_8460.png   | 41      | screenshot_8640.png   |\n",
    "| 7     | 0.003          | 171     | screenshot_32040.png  | 172     | screenshot_32220.png  |\n",
    "| 8     | 0.003          | 348     | screenshot_64080.png  | 349     | screenshot_64260.png  |\n",
    "| 9     | 0.003          | 396     | screenshot_73260.png  | 397     | screenshot_73440.png  |\n",
    "| 10    | 0.003          | 41      | screenshot_8640.png   | 42      | screenshot_8820.png   |\n",
    "\n",
    "### 7. Cleaning up the subframes\n",
    "Since the authoring tool used (<a href=\"https://github.com/TactileVision/CollabJam-Client\">CollabJam V1.1.0</a>) employs popover menus that appear centrally in the image, there were isolated cases of unwanted fluctuations in the SSIM values and visual disturbances in the generated GIF animation. These popover elements led to abrupt changes in the image, which were incorrectly interpreted by the analysis procedure as design activity.\n",
    "\n",
    "<figure id=\"Fig.7_lifetime_map\" style=\"--min-width: 15em\">\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\cleanup\\ok_frame.png\"/>\n",
    "        <figcaption>ok subframe</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\cleanup\\ok_lifetime_map.png\"/>\n",
    "        <figcaption>ok lifetime map</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\cleanup\\bad_frame.png\"/>\n",
    "        <figcaption>poor subframe</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\cleanup\\bad_lifetime_map.png\"/>\n",
    "        <figcaption>poor lifetime map</figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"Assets\\images_for_markdown\\cleanup\\bad_animation.gif\"/>\n",
    "        <figcaption>poor animation</figcaption>\n",
    "    </figure>\n",
    "    <figcaption><strong>Figure 7: Effects of poor subframes using the example of frames 241 and 242, which have the highest SSIM activity value in the generated list. </strong></figcaption>\n",
    "</figure>\n",
    "\n",
    "In order not to distort the significance of the evaluation, all subframes with exceptionally high SSIM activity values were checked manually (e.g. by using the list of SSIM activity values and the corresponding subframes. Frames in which the change could be attributed to the display of user interfaces or other interface elements were removed from the analysis (deleted from `Output\\extracted_subframes`). This ensured that only actual changes in the design work process were taken into account.\n",
    "\n",
    "For the screenrecording of the designsessions, this process was already done. Cleaned up subframes are located at `Assets\\cleaned_subframes`. To run the analysis on these frames, either switch up the subframepath or replace all extracted subframes at `Output\\extracted_subframes` by the cleaned up subframes.\n",
    "\n",
    "List of removed frames:\n",
    "- screenshot_1620.png\n",
    "- screenshot_1800.png\n",
    "- screenshot_2520.png\n",
    "- screenshot_2700.png\n",
    "- screenshot_2880.png\n",
    "- screenshot_3060.png\n",
    "- screenshot_6660.png\n",
    "- screenshot_43560.png\n",
    "- screenshot_65880.png\n",
    "- screenshot_66060.png\n",
    "- screenshot_72360.png\n",
    "\n",
    "<style>\n",
    "    figure:has(> figure) {\n",
    "    --column-gap: 1em;\n",
    "\n",
    "    display: flex;\n",
    "    flex-wrap: wrap;\n",
    "    justify-content: center;\n",
    "    align-items: baseline;\n",
    "    column-gap: var(--column-gap);\n",
    "    }\n",
    "\n",
    "    figure > figure {\n",
    "        min-width: min(var(--min-width), 100%);\n",
    "        flex-basis: 0;\n",
    "        flex-grow: 1;\n",
    "    }\n",
    "\n",
    "    figcaption {\n",
    "        width: 100%;\n",
    "        text-align: center;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c787aefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting analysis for 410 Frames...\n",
      "Processing: |██████████████████████████████████████████████████| 100.0% Complete\n",
      "SSIM-based activity-Diagram saved.\n",
      "Saved sorted list.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio.v2 as iio\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# paths\n",
    "subframes = r\"Output\\extracted_subframes\"\n",
    "mask_path = r\"Assets\\mask.png\"\n",
    "output_folder = r\"Output\"\n",
    "\n",
    "# settings for diagramm\n",
    "labelSize = 16\n",
    "\n",
    "def generate_particle_overlay_frames_generator(image_paths, mask, max_lifetime, debug=False):\n",
    "    if not image_paths:\n",
    "        return\n",
    "\n",
    "    if debug and not os.path.exists(output_folder + \"\\\\debug_frames\"):\n",
    "        os.makedirs(output_folder + \"\\\\debug_frames\")\n",
    "\n",
    "    first_img = cv2.imread(image_paths[0])\n",
    "    if first_img is None:\n",
    "        raise ValueError(f\"Could not load image: {image_paths[0]}\")\n",
    "\n",
    "    h, w, _ = first_img.shape\n",
    "\n",
    "    lifetime_map = np.zeros((h, w), dtype=np.int32)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "\n",
    "\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        img_original = cv2.imread(image_path)\n",
    "        if img_original is None:\n",
    "            print(f\"Could not load image and skipping it: {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        # grayscaling\n",
    "        img_gray = cv2.cvtColor(img_original, cv2.COLOR_BGR2GRAY)\n",
    "        img_masked = cv2.bitwise_and(img_gray, img_gray, mask=mask)\n",
    "        \n",
    "        # thresholding\n",
    "        _, thresh = cv2.threshold(img_masked, 180, 255, cv2.THRESH_BINARY_INV)\n",
    "        \n",
    "        # morphological opening\n",
    "        processed_thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        final_binary_image = cv2.bitwise_and(processed_thresh, processed_thresh, mask=mask)\n",
    "        \n",
    "        # find contours\n",
    "        contours, _ = cv2.findContours(final_binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # update lifetime map\n",
    "        lifetime_map[lifetime_map > 0] -= 1\n",
    "\n",
    "        current_frame_blocks_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        for cnt in contours:\n",
    "            cv2.drawContours(current_frame_blocks_mask, [cnt], -1, 255, -1)\n",
    "\n",
    "        lifetime_map[current_frame_blocks_mask == 255] = max_lifetime\n",
    "        \n",
    "        # all intermediate results can be saved for debugging purposes\n",
    "        if debug:\n",
    "            frame_dir = os.path.join(output_folder + \"\\\\debug_frames\", f\"frame_{i:04d}\")\n",
    "            if not os.path.exists(frame_dir):\n",
    "                os.makedirs(frame_dir)\n",
    "            \n",
    "            # safe intermediate steps\n",
    "            cv2.imwrite(os.path.join(frame_dir, \"01_masked.png\"), img_masked)\n",
    "            cv2.imwrite(os.path.join(frame_dir, \"02_thresholding.png\"), thresh)\n",
    "            cv2.imwrite(os.path.join(frame_dir, \"03_morphology.png\"), processed_thresh)\n",
    "            \n",
    "            # contours\n",
    "\n",
    "            # detected contours only\n",
    "            contours_img_black_bg = np.zeros_like(img_original)\n",
    "            cv2.drawContours(contours_img_black_bg, contours, -1, (0, 255, 0), 2)\n",
    "            cv2.imwrite(os.path.join(frame_dir, \"04.1_contours_only.png\"), contours_img_black_bg)\n",
    "\n",
    "            # detected contours overlapping with original subframe\n",
    "            contours_img = img_original.copy()\n",
    "            cv2.drawContours(contours_img, contours, -1, (0, 255, 0), 2)\n",
    "            cv2.imwrite(os.path.join(frame_dir, \"04.2_contours_with_original.png\"), contours_img)\n",
    "            \n",
    "            # lifetime map\n",
    "            lifetime_map_visual = np.clip((lifetime_map.astype(np.float32) / max_lifetime) * 255, 0, 255).astype(np.uint8)\n",
    "            cv2.imwrite(os.path.join(frame_dir, \"05_lifetime_map.png\"), lifetime_map_visual)\n",
    "        \n",
    "        # generate frame for animation\n",
    "\n",
    "        # normalization of lifetime\n",
    "        opacity_intensity_channel = (lifetime_map.astype(np.float32) / max_lifetime)\n",
    "\n",
    "        # generate black image\n",
    "        overlay_visual = np.zeros((h, w, 3), dtype=np.float32)\n",
    "\n",
    "        # pass opacity values to green channel only\n",
    "        overlay_visual[:, :, 1] = opacity_intensity_channel\n",
    "\n",
    "        # gamma / alpha correction\n",
    "        # make gradients appear more ‘natural’ and ‘less digital’\n",
    "        # particles fade out more gradually rather than linearly\n",
    "        alpha_channel_3d = opacity_intensity_channel[..., np.newaxis]\n",
    "        blended_float = overlay_visual * alpha_channel_3d\n",
    "        \n",
    "        # convert to final frame\n",
    "        blended_float = np.clip(blended_float, 0, 1) \n",
    "        blended_frame_uint8 = (blended_float * 255).astype(np.uint8)\n",
    "\n",
    "        # deliver frame of animation\n",
    "        yield blended_frame_uint8\n",
    "\n",
    "def calculate_metrics(prev_frame, current_frame):    \n",
    "    # visual activity measure based on structural dissimilarity.\n",
    "    # 0.0 = no change, higher values = stronger visual change  \n",
    "    # [:, :, 1] -> use only the green channel for faster computation, as red and blue are always zero\n",
    "    ssim_score = ssim(prev_frame[:, :, 1], current_frame[:, :, 1], channel_axis=-1, data_range=255)\n",
    "    return  1.0 - ssim_score\n",
    "\n",
    "def plot_diagram(values, type_name, ylabel, filename, color):\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(range(1, len(values) + 1), values, color=color, marker='o', linestyle='-', markersize=4)\n",
    "    plt.xlabel(\"Frame\", fontsize=labelSize)\n",
    "    plt.ylabel(ylabel, fontsize=labelSize)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, filename))\n",
    "    plt.close()\n",
    "    print(f\"{type_name}-Diagram saved.\")\n",
    "\n",
    "def save_sorted_list(differences, top_n):\n",
    "    differences.sort(key=lambda x: x['activity'], reverse=True)\n",
    "    output_lines = [\"--- Frame pairs sorted by descending difference ---\\n\"]\n",
    "    for j, diff in enumerate(differences[:top_n]):\n",
    "        activity, (f1, f2), (n1, n2) = diff['activity'], diff['pair'], diff['frame_numbers']\n",
    "        output_lines.append(f\"{j+1}. Place: visual activity = {activity:.3f}\\n   - Frame {n1}: {os.path.basename(f1)}\\n   - Frame {n2}: {os.path.basename(f2)}\\n\")\n",
    "    \n",
    "    with open(os.path.join(output_folder, \"sorted_differences.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(output_lines))\n",
    "    print(\"Saved sorted list.\")\n",
    "\n",
    "def main():\n",
    "    image_paths = sorted([\n",
    "        os.path.join(subframes, f)\n",
    "        for f in os.listdir(subframes)\n",
    "        if f.lower().endswith(('.png', '.jpg'))\n",
    "    ])\n",
    "\n",
    "    # ensure that frames are processed in the correct order\n",
    "    screenshot_number_regex = re.compile(r'^screenshot_(\\d+)\\.png$', re.IGNORECASE)\n",
    "    temp_files = []\n",
    "    for f in os.listdir(subframes):\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            match = screenshot_number_regex.search(f)\n",
    "            num = int(match.group(1)) if match else -1\n",
    "            temp_files.append((num, os.path.join(subframes, f)))\n",
    "    temp_files.sort(key=lambda x: x[0])\n",
    "    image_paths = [path for num, path in temp_files if num != -1]\n",
    "\n",
    "    total_frames = len(image_paths)\n",
    "    if total_frames == 0: raise ValueError(\"No frames found. Please first generate subframes based on video material (see section Preparation - Extraction of subframes).\")\n",
    "    \n",
    "    mask_img = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if mask_img is None: raise ValueError(\"Could not load mask.\")\n",
    "\n",
    "    # container\n",
    "    activity_values = []\n",
    "    differences_list = []\n",
    "    \n",
    "    print(f\"starting analysis for {total_frames} Frames...\")\n",
    "\n",
    "    # init generator and gif-writer\n",
    "    # if debug == True, all intermediate steps (masking, thresholding, etc.) for every frame are saved at ./Output/debug_frames\n",
    "    debug = False\n",
    "\n",
    "    # defines the number of frames required for a particle to disappear completely\n",
    "    # high value (e.g. 100) --> particles leave long, trail-like traces. This creates a strong ‘ghosting effect’. This is helpful for visualising movement paths over a longer period of time.\n",
    "    # low value (e.g. 5) --> particles disappear almost immediately. The image appears more turbulent, but shows the current state of the respective frame more precisely.\n",
    "    max_lifetime = 20\n",
    "\n",
    "    # setup generator\n",
    "    frame_gen = generate_particle_overlay_frames_generator(image_paths, mask_img, max_lifetime=max_lifetime, debug=debug)\n",
    "    gif_path = os.path.join(output_folder, \"overlay_progress.gif\")    \n",
    "    prev_frame = None\n",
    "    \n",
    "    # duration determines how long a single image (frame) is displayed before the next one appears\n",
    "    # This value can be adjusted depending on the number of frames. \n",
    "    # If the animation is too fast, increase the value (e.g. to 100 for 10 FPS). \n",
    "    # If it is too slow, decrease it (e.g. to 33 for approx. 30 FPS).\n",
    "    # FPS = 1000 / duration\n",
    "    duration = 50\n",
    "\n",
    "    with iio.get_writer(gif_path, mode='I', duration=duration, loop=0) as writer:\n",
    "        for i, current_frame in enumerate(progressBar(frame_gen, total=total_frames, prefix='Processing:', suffix='Complete', length=50)):\n",
    "            \n",
    "            # 1. calculate all metrics\n",
    "            if prev_frame is not None:\n",
    "                activity = calculate_metrics(prev_frame, current_frame)\n",
    "                activity_values.append(activity)\n",
    "                \n",
    "                # safe data for list\n",
    "                differences_list.append({\n",
    "                    'activity' : activity,\n",
    "                    'pair': (image_paths[i-1], image_paths[i]),\n",
    "                    'frame_numbers': (i-1, i)\n",
    "                })\n",
    "\n",
    "            # 2. prepare animation frame\n",
    "            gif_frame = current_frame.copy()            \n",
    "            writer.append_data(gif_frame)\n",
    "            prev_frame = current_frame.copy()\n",
    "\n",
    "    # SSIM Diagramm\n",
    "    plot_diagram(activity_values, \"SSIM-based activity\", \"visual activity (1 − SSIM)\", \"activity_diagram_ssim.svg\", \"firebrick\")\n",
    "    # safe list\n",
    "    save_sorted_list(differences_list, top_n=40)\n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
